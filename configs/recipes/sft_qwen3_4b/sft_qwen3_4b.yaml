model:
  model_name: "Qwen/Qwen3-VL-4B-Instruct"
  model_max_length: 4096  # Qwen3-VL supports up to 4096 tokens
  dtype: "bfloat16"
  use_cache: false
  load_in_4bit: true
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_compute_dtype: "bfloat16"
  bnb_4bit_use_double_quant: true
  min_pixels: 16384  # Minimum resolution: 16 * 32 * 32
  max_pixels: 589824  # Maximum resolution: 576 * 32 * 32
  freeze_vision_tower: true  # Keep vision encoder frozen during LoRA tuning

data:
  train_dataset:
    dataset_name: "yosubshin/m2sv-sft-11k"
    split: "train"
  use_dataset_validation_split: true  # Use built-in train/validation splits
  validation_split_percentage: 0.1  # Fallback if no validation split exists
  validation_seed: 42  # For reproducible train/val splits

training:
  trainer_type: "standard"  # Hugging Face Trainer for VLM
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 16  # Effective batch size: 16
  learning_rate: 2.0e-05
  lr_scheduler_type: "cosine"
  warmup_ratio: 0.03
  weight_decay: 0.01
  max_steps: 5000
  save_steps: 273
  save_strategy: "steps"
  evaluation_strategy: "steps"  # Enable validation during training
  eval_steps: 273  # Run validation once per epoch (~273 steps for 4370 examples)
  logging_steps: 10
  save_total_limit: 3
  load_best_model_at_end: true  # Load best checkpoint based on eval_loss
  metric_for_best_model: "eval_loss"  # Use validation loss to select best model
  greater_is_better: false  # Lower eval_loss is better
  output_dir: "./train/results/local/qwen3_vl_4b_lora_m2sv"
  bf16: true
  gradient_checkpointing: true
  optim: "adamw_torch"
  report_to: "wandb"

peft:
  type: "lora"
  lora_r: 16
  lora_alpha: 32
  lora_dropout: 0.0
  lora_target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
  bias: "none"
