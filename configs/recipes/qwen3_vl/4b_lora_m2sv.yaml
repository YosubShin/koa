# Qwen3-VL 4B with LoRA training on M2SV-SFT dataset
# Vision-Language Model for street-view to map matching task
# Memory efficient: ~20-24GB GPU memory with LoRA

model:
  model_name: "Qwen/Qwen3-VL-4B-Instruct"
  model_max_length: 2048  # Qwen3-VL context length
  dtype: "bfloat16"
  use_cache: false

  # QLoRA enabled for lower memory usage (RTX 2080 Ti has 11GB)
  load_in_4bit: true
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_compute_dtype: "bfloat16"
  bnb_4bit_use_double_quant: true

data:
  train_dataset:
    dataset_name: "yosubshin/m2sv-sft"
    split: "train"
    # Limited to 10 examples for quick testing
    limit: 10

training:
  trainer_type: "standard"  # Using HF Trainer for VLM
  per_device_train_batch_size: 1  # Reduced from 2 for 11GB GPU
  gradient_accumulation_steps: 16  # Effective batch size: 1 * 16 = 16
  learning_rate: 2.0e-04
  lr_scheduler_type: "cosine"
  warmup_ratio: 0.03
  max_steps: 100  # Quick test run
  save_steps: 50
  logging_steps: 10
  save_total_limit: 2
  output_dir: "./train/results/local/qwen3_vl_4b_lora_m2sv"
  bf16: true
  gradient_checkpointing: true
  optim: "adamw_torch"  # Fused version didn't help with QLoRA

  # Logging - W&B disabled temporarily due to permission issues
  report_to: "none"  # TODO: Fix W&B permissions and re-enable

peft:
  type: "lora"
  lora_r: 16
  lora_alpha: 32
  lora_dropout: 0.05
  # Target modules for Qwen3-VL vision-language model
  lora_target_modules:
    - "q_proj"
    - "v_proj"
    - "k_proj"
    - "o_proj"
  bias: "none"
