# Llama 3.1 8B full fine-tuning
# No LoRA - trains all parameters
# Requires multiple GPUs or FSDP for memory efficiency
# WARNING: Requires significant GPU memory (40GB+ per GPU)

model:
  model_name: "meta-llama/Llama-3.1-8B-Instruct"
  model_max_length: 8192
  dtype: "bfloat16"
  attn_implementation: "flash_attention_2"
  use_cache: false

data:
  train_dataset:
    dataset_name: "yahma/alpaca-cleaned"
    split: "train"
  target_column: "text"
  template: "llama3"

training:
  trainer_type: "sft"
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 64  # Effective batch size: 64
  learning_rate: 5.0e-06  # Lower LR for full fine-tuning
  lr_scheduler_type: "cosine"
  warmup_ratio: 0.03
  num_train_epochs: 3
  save_steps: 100
  logging_steps: 5
  save_total_limit: 2
  output_dir: "./output/llama8b_full"
  bf16: true
  gradient_checkpointing: true
  optim: "adamw_torch_fused"

  # Multi-GPU / Distributed training
  fsdp: "full_shard auto_wrap"  # Fully Sharded Data Parallel
  fsdp_transformer_layer_cls_to_wrap: "LlamaDecoderLayer"

  # Evaluation
  eval_strategy: "steps"
  eval_steps: 100

  # Logging
  report_to: "none"

# No peft section = full fine-tuning
