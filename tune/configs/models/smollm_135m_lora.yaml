# SmolLM 135M with LoRA fine-tuning
# Perfect for quick testing on KOA - trains in minutes

model:
  model_name: "HuggingFaceTB/SmolLM2-135M-Instruct"
  model_max_length: 2048
  dtype: "bfloat16"
  attn_implementation: "sdpa"  # Scaled Dot Product Attention (built-in PyTorch)

data:
  train_dataset:
    dataset_name: "yahma/alpaca-cleaned"
    split: "train[:1000]"  # Use only 1000 examples for quick testing
  target_column: "text"
  template: "alpaca"  # Standard Alpaca prompt template

training:
  trainer_type: "sft"  # Supervised Fine-Tuning
  per_device_train_batch_size: 4
  gradient_accumulation_steps: 4
  learning_rate: 2.0e-04
  lr_scheduler_type: "cosine"
  warmup_ratio: 0.03
  max_steps: 100  # Quick test - ~5-10 minutes
  save_steps: 50
  logging_steps: 10
  save_total_limit: 2
  output_dir: "./output/smollm_lora_test"
  bf16: true
  gradient_checkpointing: true
  optim: "adamw_torch_fused"

  # Logging
  report_to: "none"  # Change to "wandb" if you want W&B logging

peft:
  type: "lora"
  lora_r: 8
  lora_alpha: 16
  lora_dropout: 0.05
  lora_target_modules: ["q_proj", "v_proj", "k_proj", "o_proj"]
  bias: "none"
  task_type: "CAUSAL_LM"
