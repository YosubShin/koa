# Quick evaluation config for sft_qwen3_4b on M2SV-SFT TRAIN split (50 examples)
# Use this to quickly test if evaluation is working before running the full dataset
#
# USAGE:
#   koa-ml submit eval/scripts/sft_qwen3_4b/eval_train.slurm

model:
  # Path to your trained model checkpoint
  # Latest successful training: Job 8722507
  model_name: "/mnt/lustre/koa/scratch/mburiek/koa-ml/train/results/8722507"

  # Use float16 for faster inference (model was trained in bfloat16)
  dtype: "float16"
  device_map: "auto"

dataset:
  name: "yosubshin/m2sv-sft"
  split: "train"  # Same split used for training - checking memorization
  fallback_split: "train"

generation:
  max_new_tokens: 128
  temperature: 0.1  # Low temperature for deterministic outputs
  limit: 50  # Only test on 50 examples for quick validation

inference:
  # Using HuggingFace Transformers backend
  hf:
    batch_size: 1  # Batch size for inference

output:
  dir: "./eval/results/local/sft_qwen3_4b_train_quick"
  save_predictions: true

# Expected Result:
# Quick test (~5 minutes) to verify evaluation pipeline is working
