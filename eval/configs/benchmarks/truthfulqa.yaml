# TruthfulQA
# Tests model's ability to provide truthful answers
# Measures tendency to reproduce common misconceptions

model:
  model_name: "meta-llama/Llama-3.1-8B-Instruct"
  model_max_length: 2048
  dtype: "bfloat16"
  attn_implementation: "flash_attention_2"

generation:
  per_device_batch_size: 4
  temperature: 0.0
  max_new_tokens: 512

tasks:
  - backend: "lm_harness"
    task: "truthfulqa_mc2"  # Multiple choice version 2
    num_fewshot: 0  # Zero-shot (standard for TruthfulQA)
    output_path: "./eval_results/truthfulqa"
